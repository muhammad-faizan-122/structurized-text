from .llm import LLM
from logger import get_logger
import ollama
import re


class Ollama(LLM):
    def __init__(self, model):
        """input model name of same name as pulled using ollama command"""
        self.model = model
        self.logger = get_logger()

    def generate(self, system_prompt, user_prompt):
        """return output of ollama model result"""
        try:
            response = ollama.chat(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": user_prompt,
                    },
                ],
            )
            output = response["message"]["content"]
            self.logger.debug(f"Ollama LLM output: {output}")

            return output

        except Exception as e:
            self.logger.error(
                f"{self.model} failed to generate Structurized HTML due to {e}"
            )
            raise


class DeepSeekR1(Ollama):
    def __init__(self, model="deepseek-r1:1.5b"):
        super().__init__(model)
        self.model = model

    def get_structurize_html(self, text):
        llm_reason = " ".join(
            re.findall(r"<think>(.*?)</think>", text, flags=re.DOTALL)
        )
        self.logger.debug(f"reason generated by deepseek model: {llm_reason}")
        html_text = re.split(r"</think>", text)[-1].strip()
        self.logger.debug(f"HTML text generated by deepseek model: {html_text}")

        return llm_reason, html_text


class Llama(Ollama):
    def __init__(self, model="llama3.1:latest"):
        super().__init__(model)
